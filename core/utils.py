import os
import json
import random
import requests
import base64
import io
import torch
import cv2
import tempfile
import numpy as np
import urllib3
import copy
import re
from PIL import Image
from enum import Enum
import logging

# ÂÜÖÁΩÆÈªòËÆ§Ê®°ÂûãÔºàÁ°¨ÁºñÁ†ÅÔºå‰Ωú‰∏∫ÊúÄÂêéÁöÑÂêéÂ§áÂíåÊñá‰ª∂ÁîüÊàêÊ∫êÔºâ
_BUILTIN_DEFAULT_MODELS = {
    "Gemini": ["[CHAT] gemini-1.5-flash","[CHAT] gemini-1.5-pro","[VISION] gemini-2.0-flash-exp"],
    "OpenAI": ["[CHAT] gpt-4o","[CHAT] gpt-4o-mini","[IMAGE] dall-e-3"],
    "Grok": ["[CHAT] grok-2-latest","[CHAT] grok-beta"],
    "Qwen": ["[VISION] qwen-vl-max","[CHAT] qwen-turbo","[CHAT] qwen-plus"],
    "Doubao": ["[CHAT] doubao-pro-32k","[IMAGE] doubao-t2i-pro"],
    "Hailuo": ["[VIDEO] mini-max-v1"],
    "Luma": ["[VIDEO] luma-ray-v1"],
    "DeepSeek": ["[CHAT] deepseek-chat","[CHAT] deepseek-coder"]
}

# ÈÖçÁΩÆÊó•ÂøóÔºàÂ¶ÇÊûúÂ∞öÊú™ÈÖçÁΩÆÔºâ
logger = logging.getLogger(__name__)

# ÈªòËÆ§Ê®°ÂûãÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
DEFAULT_MODELS_PATH = os.path.join(os.path.dirname(__file__), "default_models.json")


def ensure_default_models_file():
    """
    Â¶ÇÊûú default_models.json ‰∏çÂ≠òÂú®ÔºåÂàôÁî®ÂÜÖÁΩÆÈªòËÆ§Ê®°ÂûãÂàõÂª∫„ÄÇ
    Ëã•Êñá‰ª∂Â∑≤Â≠òÂú®‰ΩÜÂÜÖÂÆπÊçüÂùèÔºåÂèØÈÄâÊã©ÊÄß‰øÆÂ§çÔºàËøôÈáåÁÆÄÂçïËÆ∞ÂΩïÈîôËØØ‰ΩÜ‰∏çË¶ÜÁõñÔºâ„ÄÇ
    """
    if os.path.exists(DEFAULT_MODELS_PATH):
        # ÂèØÈÄâÔºöÈ™åËØÅÊñá‰ª∂ÂÜÖÂÆπÊòØÂê¶‰∏∫ÊúâÊïà JSON ‰∏î‰∏∫Â≠óÂÖ∏ÔºåËã•‰∏çÊòØÂàôËÆ∞ÂΩïË≠¶Âëä
        try:
            with open(DEFAULT_MODELS_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not isinstance(data, dict):
                logger.warning(f"{DEFAULT_MODELS_PATH} exists but is not a dictionary. "
                               "You may want to delete it to regenerate.")
        except Exception as e:
            logger.error(f"Failed to parse {DEFAULT_MODELS_PATH}: {e}")
        return  # Êñá‰ª∂Â∑≤Â≠òÂú®Ôºå‰∏çË¶ÜÁõñ

    # Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåÂàõÂª∫
    try:
        with open(DEFAULT_MODELS_PATH, "w", encoding="utf-8") as f:
            json.dump(_BUILTIN_DEFAULT_MODELS, f, indent=4, ensure_ascii=False)
        logger.info(f"Created default models file: {DEFAULT_MODELS_PATH}")
    except Exception as e:
        logger.error(f"Failed to create default models file: {e}")

# ÁºìÂ≠òÈªòËÆ§Ê®°ÂûãÂ≠óÂÖ∏ÔºåÈÅøÂÖçÈáçÂ§çËØªÂèñÊñá‰ª∂
_DEFAULT_MODELS_CACHE = None

def load_default_models():
    """Âä†ËΩΩÈªòËÆ§Ê®°ÂûãÈÖçÁΩÆÊñá‰ª∂ÔºåËøîÂõû {provider: [model_with_tag]} Â≠óÂÖ∏"""
    global _DEFAULT_MODELS_CACHE
    if _DEFAULT_MODELS_CACHE is not None:
        return _DEFAULT_MODELS_CACHE

    # Á°Æ‰øùÊñá‰ª∂Â≠òÂú®ÔºàÁêÜËÆ∫‰∏äÂ∑≤Âú®Ê®°ÂùóÂä†ËΩΩÊó∂Á°Æ‰øùÔºå‰ΩÜÊ≠§Â§ÑÂÜçÊ¨°Ê£ÄÊü•‰ª•Èò≤Â§ñÈÉ®Âà†Èô§Ôºâ
    ensure_default_models_file()

    try:
        with open(DEFAULT_MODELS_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            if not isinstance(data, dict):
                raise ValueError("default_models.json must be a dictionary")
            _DEFAULT_MODELS_CACHE = data
            return data
    except Exception as e:
        logger.error(f"Failed to load default models, using built-in defaults: {e}")
        _DEFAULT_MODELS_CACHE = _BUILTIN_DEFAULT_MODELS.copy()
        return _DEFAULT_MODELS_CACHE

# ====================== ÂÖ®Â±ÄÈÖçÁΩÆ ======================
VERIFY_SSL = False
if not VERIFY_SSL:
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

api_session = requests.Session()
api_session.verify = VERIFY_SSL

CACHE_PATH = os.path.join(os.path.dirname(__file__), "universal_model_cache.json")
_GLOBAL_AI_CONFIG = {}

# ====================== ËÉΩÂäõÊûö‰∏æ ======================
class ModelCapability(Enum):
    CHAT = "chat"
    VISION = "vision"
    IMAGE_GEN = "image_gen"
    VIDEO_GEN = "video_gen"
    AUDIO_GEN = "audio_gen"
    UNKNOWN = "unknown"

# ====================== Ê†∏ÂøÉËÉΩÂäõÂà§Êñ≠ ======================
def get_model_capability(model_name: str, provider: str = "") -> ModelCapability:
    """
    Ê†πÊçÆÊ®°ÂûãÂêçÁß∞ÂíåÊèê‰æõÂïÜÂà§Êñ≠Ê®°ÂûãËÉΩÂäõÔºà‰ºòÂÖàÁ∫ßÔºöËßÜÈ¢ë > ÂõæÂÉè > Èü≥È¢ë > ËßÜËßâ > ÂØπËØùÔºâ
    """
    if not isinstance(model_name, str):
        return ModelCapability.UNKNOWN

    name_lower = model_name.lower()

    # ËßÜÈ¢ëÁîüÊàê
    if any(kw in name_lower for kw in ["video", "sora", "cogvideo", "veo", "wan2", "t2v"]):
        return ModelCapability.VIDEO_GEN

    # Èü≥È¢ëÁîüÊàê
    if any(kw in name_lower for kw in ["audio", "speech", "tts", "whisper", "cosyvoice"]):
        return ModelCapability.AUDIO_GEN

    # ÂõæÂÉèÁîüÊàêÔºàÊéíÈô§ËßÜËßâÁêÜËß£Ê®°ÂûãÔºâ
    img_kws = ["image", "imagen", "wanx", "dall-e", "flux", "paint", "draw", "art", "gen", "style"]
    if any(kw in name_lower for kw in img_kws):
        # Â¶ÇÊûúÂêåÊó∂ÂåÖÂê´ËßÜËßâÂÖ≥ÈîÆËØçÔºàvl/visionÔºâÔºå‰ºòÂÖàËßÜ‰∏∫ËßÜËßâÁêÜËß£
        if not any(vision_kw in name_lower for vision_kw in ["vl", "vision", "visual"]):
            return ModelCapability.IMAGE_GEN

    # ËßÜËßâÁêÜËß£ÔºàÂõæÊñáËæìÂÖ•Ôºâ
    if any(kw in name_lower for kw in ["vision", "vl", "visual"]):
        return ModelCapability.VISION

    # ÈªòËÆ§ÂØπËØù
    return ModelCapability.CHAT

# ====================== Ê†áÁ≠æÁîüÊàê ======================
def get_model_tag(model_name: str, provider: str = "") -> str:
    """ËøîÂõûÂ∏¶ UI Ê†áÁ≠æÁöÑÊ®°ÂûãÂêçÁß∞ÔºåÂ¶Ç '[CHAT] qwen-max'"""
    capability = get_model_capability(model_name, provider)
    tag_map = {
        ModelCapability.CHAT: "[CHAT]",
        ModelCapability.VISION: "[VISION]",
        ModelCapability.IMAGE_GEN: "[IMAGE]",
        ModelCapability.VIDEO_GEN: "[VIDEO]",
        ModelCapability.AUDIO_GEN: "[AUDIO]",
        ModelCapability.UNKNOWN: "[UNKNOWN]",
    }
    prefix = tag_map.get(capability, "[UNKNOWN]")
    return f"{prefix} {model_name}"

def strip_model_label(model_name: str) -> str:
    """ÁßªÈô§Ê®°ÂûãÂêçÁß∞ÂºÄÂ§¥ÁöÑÊ†áÁ≠æÂâçÁºÄÔºå‰æãÂ¶Ç '[CHAT] '"""
    if not isinstance(model_name, str):
        return ""
    return re.sub(r'^\[\w+\]\s*', '', model_name)

# ====================== ÂéüÊúâÂ∑•ÂÖ∑ÂáΩÊï∞‰øùÊåÅ‰∏çÂèò ======================
def parse_extra_params(extra_str):
    """Ëß£Êûê extra_params JSON Â≠óÁ¨¶‰∏≤"""
    try:
        return json.loads(extra_str) if extra_str.strip() else {}
    except:
        return {}

def get_api_key(api_key_str):
    """ÊîØÊåÅÈÄóÂè∑ÂàÜÈöîÁöÑÂ§ö Key ÈöèÊú∫ËΩÆËØ¢"""
    if not api_key_str:
        return ""
    keys = [k.strip() for k in api_key_str.split(",") if k.strip()]
    return random.choice(keys) if keys else ""

def extract_all_text(parts):
    """‰ªé parts ‰∏≠ÊèêÂèñÊâÄÊúâÊñáÊú¨"""
    texts = [p["data"] for p in parts if p["type"] == "text"]
    return "\n\n".join(texts)

def extract_all_images(parts):
    """‰ªé parts ‰∏≠ÊèêÂèñÊâÄÊúâÂõæÁâáÊï∞ÊçÆÔºàBase64Ôºâ"""
    return [p["data"] for p in parts if p["type"] == "image"]

def safe_process_image(img_data):
    """ÂÆâÂÖ®Â§ÑÁêÜÂõæÁâáÊï∞ÊçÆÔºåË°•ÂÖ® Data URI ÂâçÁºÄ"""
    if not isinstance(img_data, str):
        print(f"‚ö†Ô∏è [Universal AI] Warning: Expected Base64 string, but got {type(img_data)}.")
        return None
    clean_data = img_data.replace("\n", "").replace("\r", "").strip()
    if clean_data.startswith("data:image"):
        return clean_data
    return f"data:image/jpeg;base64,{clean_data}"

# utils.pyÔºà‰øÆÊîπÂêéÁöÑ sync_all_models ÂáΩÊï∞Ôºâ
def sync_all_models(provider, api_key):
    """Âà∑Êñ∞Ê®°ÂûãÂàóË°®Ôºå‰ΩøÁî®ÂêåÊ≠•Âô®Â∑•ÂéÇËß£ËÄ¶"""
    if not api_key:
        return

    from .syncers.modelSyncerFactory import ModelSyncerFactory

    syncer = ModelSyncerFactory.get_syncer(provider, api_key)
    if not syncer:
        print(f"‚ö†Ô∏è [Universal AI] No syncer found for provider: {provider}")
        return

    collected_models = syncer.sync()
    if not collected_models:
        print(f"‚ö†Ô∏è [Universal AI] No models synced for {provider}")
        return

    # ÁºìÂ≠òÂÜôÂÖ•Ôºà‰∏éÂéüÈÄªËæëÁõ∏ÂêåÔºâ
    try:
        cache_data = {}
        if os.path.exists(CACHE_PATH):
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                try:
                    cache_data = json.load(f)
                except:
                    cache_data = {}
        unique_models = sorted(list(set(collected_models)))
        cache_data[provider] = unique_models
        with open(CACHE_PATH, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, indent=4, ensure_ascii=False)
        print(f"üíæ [Universal AI] {provider} cache updated with {len(unique_models)} items.")
    except Exception as e:
        print(f"‚ùå [Universal AI] Cache Write Error: {e}")

def get_combined_models(provider=None):
    """
    Ëé∑ÂèñÂêàÂπ∂ÁöÑÊ®°ÂûãÂàóË°®ÔºàÁºìÂ≠ò + ÈªòËÆ§Ê®°ÂûãÔºâ
    Â¶ÇÊûú provider ‰∏∫ NoneÔºåÂàôËøîÂõûÊâÄÊúâÊèê‰æõÂïÜÁöÑÊ®°ÂûãÔºàÁî®‰∫é‰∏ãÊãâÊ°ÜÂÖ®ÈáèÂ±ïÁ§∫Ôºâ
    """
    default_models = load_default_models()
    # ... ÂÖ∂‰ΩôÈÄªËæë‰∏çÂèòÔºå‰ΩÜÂêéÂ§áÂèØ‰ª•ËÆæ‰∏∫Á©∫ÂàóË°®ÊàñÁúÅÁï•
    # Â¶ÇÊûúÊ≤°Êúâ‰ªª‰ΩïÊ®°ÂûãÔºåÂèØËøîÂõû [] Êàñ‰∏Ä‰∏™ÈÄöÁî®ÂêéÂ§á

    # ËØªÂèñÁºìÂ≠ò
    cache = {}
    if os.path.exists(CACHE_PATH):
        try:
            with open(CACHE_PATH, "r", encoding="utf-8") as f:
                cache = json.load(f)
                if not isinstance(cache, dict):
                    cache = {}
        except Exception:
            cache = {}

    if provider:
        # Âçï‰∏™Êèê‰æõÂïÜÔºö‰ºòÂÖà‰ΩøÁî®ÁºìÂ≠òÔºåËã•Êó†Âàô‰ΩøÁî®ÈªòËÆ§ÔºåËã•Êó†ÈªòËÆ§ÂàôÁî®ÂêéÂ§á
        provider_models = cache.get(provider, [])
        if not provider_models:
            provider_models = default_models.get(provider, fallback_defaults)
        return sorted(set(provider_models))
    else:
        # ÂÖ®ÈáèÊ®°ÂûãÔºöÂêàÂπ∂ÊâÄÊúâÁºìÂ≠ò + ÊâÄÊúâÈªòËÆ§
        all_models = set()
        for models in cache.values():
            all_models.update(models)
        for models in default_models.values():
            all_models.update(models)
        # Â¶ÇÊûú‰∏∫Á©∫ÔºåËá≥Â∞ëËøîÂõûÂêéÂ§áÂàóË°®
        if not all_models:
            return fallback_defaults
        return sorted(all_models)

def tensor_to_base64(tensor, max_size=1024, auto_resize=True):
    """Â∞Ü ComfyUI Tensor ËΩ¨Êç¢‰∏∫ Base64 Â≠óÁ¨¶‰∏≤ÔºåÊîØÊåÅËá™Âä®Áº©Êîæ"""
    if tensor.ndim == 4:
        tensor = tensor[0]
    img_np = (255. * tensor.cpu().numpy()).clip(0, 255).astype(np.uint8)
    img = Image.fromarray(img_np)
    if auto_resize and max(img.size) > max_size:
        scale = max_size / max(img.size)
        img = img.resize((int(img.size[0] * scale), int(img.size[1] * scale)), Image.LANCZOS)
    buf = io.BytesIO()
    img.save(buf, format="JPEG", quality=85)
    return base64.b64encode(buf.getvalue()).decode('utf-8')

def base64_to_tensor(b64):
    """Â∞Ü Base64 ÂõæÁâáËΩ¨Êç¢‰∏∫ ComfyUI Tensor"""
    img_data = base64.decodebytes(b64.encode('utf-8'))
    img = Image.open(io.BytesIO(img_data)).convert("RGB")
    return torch.from_numpy(np.array(img).astype(np.float32) / 255.0)[None,]

def url_to_video_tensor(url):
    """‰ªéËßÜÈ¢ë URL ‰∏ãËΩΩÂπ∂Ëß£Á†Å‰∏∫Â∏ßÂº†Èáè"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        try:
            with requests.get(url, stream=True, timeout=60, verify=False) as r:
                r.raise_for_status()
                for chunk in r.iter_content(8192):
                    if chunk:
                        tmp.write(chunk)
            tmp_path = tmp.name
        except:
            return None
    try:
        cap = cv2.VideoCapture(tmp_path)
        frames = []
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0)
        cap.release()
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
        return torch.from_numpy(np.array(frames)) if frames else None
    except:
        return None

def set_global_ai_config(key: str, config):
    global _GLOBAL_AI_CONFIG
    if not key:
        return
    clean_key = key.strip()
    _GLOBAL_AI_CONFIG[clean_key] = config
    print(f"üì° [Universal AI] Config stored under key: {clean_key}")

def get_global_ai_config(key: str):
    global _GLOBAL_AI_CONFIG
    config = _GLOBAL_AI_CONFIG.get(key.strip())
    return copy.deepcopy(config) if config else None

def get_all_active_config_keys():
    global _GLOBAL_AI_CONFIG
    return list(_GLOBAL_AI_CONFIG.keys())



#Âä†ËΩΩÈªòËÆ§Ê®°ÂûãÂàóË°®ÔºåÂøÖÈ°ª
ensure_default_models_file()